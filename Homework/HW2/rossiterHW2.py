from bs4 import BeautifulSoup
import urllib2
import re
import csv


## Function to clean data for .csv
def createRow(link, text, judge_html, page):
	## getting link to pdf
	doc_in_this_div = judge_html.find("div", {"id" : "ContentW"})
	try:
		doc_link = doc_in_this_div.find("a", {"class" : "tbprint"})["href"]
	except:
		doc_link = "NA"

	## getting name of judge
	name = link.replace("http://www.judicialwatch.org/document-archive/", "")
	name = name[:-1]
	name = ''.join([i for i in name if not i.isdigit()])
	name = name.replace("-", " ")

	## getting year of report
	year = re.findall(r'\d+', link)
	try:
		year = int(year[0])
	except:
		year = 0

	row = ({"Page" : page, "LinktoDoc" : doc_link, "Name": name, "Year" : year, "Text" : text })
	return row


## Function to parse the judge/year url, get the text they've
## autogenerated, and then working that text into a list so
## it's more useful.
def getJudgeText(link):
	web_page = urllib2.urlopen(link)
	judge_html = BeautifulSoup(web_page.read(), "html.parser")

	try:
		text = judge_html.find("pre").get_text()
	except AttributeError:
		text = "NA"
	finally:
		pass

	## I read the text into a .txt file b/c python was
	## treating it as one string, but as a .txt file,
	## I can read in lines and find sections better w/in
	## the text.
	write_temp_file = open("output.txt", "w")
	try:
		write_temp_file.write(text)
	except:
		write_temp_file.write("NA")
	write_temp_file.close()

	read_temp_file = open("output.txt", "r")
	lines = read_temp_file.readlines()
	text_list = []
	for l in lines:
		text_list.append(l)
	read_temp_file.close()

	return judge_html, text_list


## Function to parse each 'page' HTML and create a list
## of all links to judge/year url's.
def getPageLinks(web_page):
	all_html = BeautifulSoup(web_page.read(), "html.parser")
	fin_disclosures = all_html.find("div", {"id" : "fin_disclosures"})
	html_to_links = fin_disclosures.find_all("span", {"class" : "title"})

	all_links = []
	for l in html_to_links:
		all_links.append(l.find("a", href=True)["href"])
	return all_links


def main():
	with open('textWork.csv', 'ab') as g:
		## Setting up csv.  Not sure what I want to do with data,
		## so I collect info I might need in future.
		w = csv.DictWriter(g, fieldnames=("Page", "LinktoDoc", "Name", "Year", "Text"))
		w.writeheader()
		## I saw in source that there are 1316 pages.  This
		## was by far the best way to thoroughly get all available data.
		for page in range(1, 3):
			start_address = "http://www.judicialwatch.org/judicial-financial-disclosure/page/%s/" % page
			web_page = urllib2.urlopen(start_address)
			page_links = getPageLinks(web_page)
			## Looping over the list of links I get from
			## getPageLinks(), running it through my
			## getJudgeText() function, running that output
			## through my createRow() function, and then
			## writing that output to my .csv
			for link in page_links:
				judge_html, text = getJudgeText(link)
				row = createRow(link, text, judge_html, page)
				w.writerow(row)


main()

  	
